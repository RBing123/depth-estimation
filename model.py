# -*- coding: utf-8 -*-
"""「model.ipynb」

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uvdnkP5Ilk3ziVaS3UM15aSbzEBDQqmI
"""

# Commented out IPython magic to ensure Python compatibility.
#   %%capture
!pip install tensorflow_addons
!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fdFu5NGXe4rTLYKD5wOqk9dl-eJOefXo' -O nyu_data.zip

from zipfile import ZipFile
import pandas as pd
import os, time
import shutil
import sys
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from tensorflow import keras
import random

#Set seed value
seed_value = 43

os.environ['PYTHONHASHSEED'] = str(seed_value)

random.seed(seed_value)

#numpy seed
np.random.seed(seed_value)

#Tf seed
tf.random.set_seed(seed_value)

#Configure new global tensorflow session
from tensorflow.compat.v1.keras import backend as k
session_conf = tf.compat.v1.ConfigProto(
    intra_op_parallelism_threads = 1,
    inter_op_parallelism_threads = 1
)

sess = tf.compat.v1.Session(graph = tf.compat.v1.get_default_graph(), config = session_conf)
k.set_session(sess)

from google.colab import drive
drive.mount('/content/drive')
def extract_zip(src, dest):
    '''
    Extracts the contents downloaded from the URL
    '''
    zip_ref = ZipFile(src,'r')
    zip_ref.extractall(dest)
    zip_ref.close()

extract_zip('/content/drive/MyDrive/vkitti_2.0.3_depth.zip', '/content')
extract_zip('/content/drive/MyDrive/vkitti_2.0.3_rgb.zip', '/content')
#extract_zip('/content/drive/MyDrive/vkitti_2.0.3_normal.zip', '/content')

#Hyper params
split = 0.8
height, width = 192, 640

path1='/content/vkitti_2.0.3_rgb'
rgb_file=os.listdir(path1)
filelist=[]
for subfile in rgb_file:
  address=path1+'/'+subfile
  for sub_subfile in os.listdir(address):
    a=path1+'/'+subfile+'/'+sub_subfile+'/'+'frames'+'/'+'rgb'
    for last in os.listdir(a):
      aa=a+'/'+last
      filelist.append(aa)
RGB_image=[]
for dirs in filelist:
  ad=os.listdir(dirs)
  ad.sort()
  for sec_dirs in ad:
    d=dirs+'/'+sec_dirs
    RGB_image.append(d)
print(ad)
path2='/content/vkitti_2.0.3_depth'
depth_file=os.listdir(path2)
dfilelist=[]
for subfile in depth_file:
  address=path2+'/'+subfile
  for sub_subfile in os.listdir(address):
    a=path2+'/'+subfile+'/'+sub_subfile+'/'+'frames'+'/'+'depth'
    for last in os.listdir(a):
      aa=a+'/'+last
      dfilelist.append(aa)
Depth_image=[]
for dirs in dfilelist:
  ad=os.listdir(dirs)
  ad.sort()
  for sec_dirs in ad:
    d=dirs+'/'+sec_dirs
    Depth_image.append(d)

'''
path3='/content/vkitti_2.0.3_normal'
normal_file=os.listdir(path3)
nfilelist=[]
for subfile in normal_file:
  address=path3+'/'+subfile
  for sub_subfile in os.listdir(address):
    a=path3+'/'+subfile+'/'+sub_subfile+'/'+'frames'+'/'+'normal'
    for last in os.listdir(a):
      aa=a+'/'+last
      nfilelist.append(aa)
normal_image=[]
for dirs in nfilelist:
  ad=os.listdir(dirs)
  ad.sort()
  for sec_dirs in ad:
    d=dirs+'/'+sec_dirs
    normal_image.append(d)
'''
'''
print(RGB_image[0])
print(Depth_image[0])
data={
      "image":[x for x in RGB_image],
      "depth":[x for x in Depth_image],
      }
'''
print(RGB_image[0])
print(Depth_image[0])
data={
      "image":[x for x in RGB_image],
      "depth":[x for x in Depth_image],
      }
df=pd.DataFrame(data)
print(df['image'][0])
print(df['depth'][0])
print(df['image'][1])
print(df['depth'][1])
df=df.sample(frac=1,random_state=42)
train_df = df.sample(frac=1).reset_index(drop=True)
train_split = int(len(train_df)*split)
train = train_df[:train_split]
validation_split=train_split+int(len(train_df)*0.1)
validation = train_df[train_split:validation_split]
test=train_df[validation_split:]
print(train_df['image'][0])
print(train_df['depth'][0])
print(train_df['image'][1])
print(train_df['depth'][1])
print(len(train), len(validation), len(test))

class DataGenerator(tf.keras.utils.Sequence):
  def __init__(self, dataframe, batch_size, shuffle=False, dim=(640,192)):
    # for reproducibility
    np.random.seed(43)
    # dataframe containing the subset of image and depth pairs
    self.df = dataframe
    # chosen Height and Width of the RGB image
    self.height, self.width = dim
    # choice of shuffling the data
    self.shuffle = shuffle
    self.batch_size = batch_size
    # unique set of RGB images
    self.ids = dataframe['image'].unique()
    # Map the image with depth maps
    self.imgtodpth = dataframe.set_index('image')['depth'].to_dict()####
    self.on_epoch_end()

  def __len__(self):
    '''
    Returns the length of dataset.
    '''
    return len(self.df) // self.batch_size

  def on_epoch_end(self):
    '''
    Shuffles the data at the end of every epoch
    '''
    self.indexes = np.arange(len(self.ids))
    if self.shuffle:
      np.random.shuffle(self.indexes)

  def __getitem__(self,index):
    '''
    returns the batch of image and depth pairs
    '''
    # select the batch of pair indexes
    idxs = self.indexes[index*self.batch_size : (index+1)*self.batch_size]
    # randomly select whether to flip the image
    flip = np.random.choice([True, False])
    # select the image id's for the above indexes
    query_imgs = [self.ids[idx] for idx in idxs]
    # select corresponding depth pair for the image
    target_imgs = [self.imgtodpth[img] for img in query_imgs]
    # preprocess the image
    processed_query_img = self._preprocess_image(query_imgs, flip)
    # preprocess the depth map
    processed_depth_img = self._preprocess_depth(target_imgs, flip)
    return processed_query_img, processed_depth_img

  def _preprocess_image(self,images, flip):
    '''
    Resize, Normalize and randomly Augments the image set.
    '''
    # placeholder for storing the processed images
    processed = []
    for img in images:
      # resize the image to 640x192
      resized_img = cv2.resize(cv2.imread(img),(self.height,self.width)).astype(np.float32)
      # normalize the image to {0,1}
      scaled_img = (resized_img - resized_img.min()) / (resized_img.max() - resized_img.min())
      # flip the image horizontally
      if flip:
        scaled_img = cv2.flip(scaled_img, 1)
      # finally append each image
      processed.append(scaled_img)
    return np.array(processed)

  def _preprocess_depth(self,images, flip):
    '''
    Resize, Normalize and randomly Augments the depth maps.
    '''
    # placeholder for storing the processed depth maps
    processed = []
    for img in images:
      # resize the depth map to 640x192
      resized_img = cv2.resize(cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY),(640,192)).astype(np.float32)#cv2.COLOR_BGR2GRAY
      #########resized_img = cv2.resize(cv2.imread(img),(self.height,self.width)).astype(np.float32)
      # normalize it to range {0,1}
      scaled_img = (resized_img - resized_img.min()) / (resized_img.max() - resized_img.min())
      # flip the image horizontally
      if flip:
        scaled_img = cv2.flip(scaled_img, 1)
      # add the color channel as cv2 grayscale image doesnt contain color channel but tensorflow expects it
      scaled_img = np.expand_dims(scaled_img, axis=-1)#######
      # finally append the image
      processed.append(scaled_img)
    return np.array(processed)

train_generator = DataGenerator(train, batch_size=8, shuffle=True, dim=(640,192))
val_generator = DataGenerator(validation, batch_size=8, shuffle=False, dim=(640,192))
test_generator = DataGenerator(test, batch_size=16, shuffle=False, dim=(640,192))
print(len(train_generator), len(val_generator), len(test_generator))

images,depths = next(iter(val_generator))
print(images.shape, depths.shape)

cmap = "plasma_r"
plt.figure(figsize=(15,9))
for i in range(0,4,2):
  image = images[i]
  depth = depths[i]
  image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  #depth=cv2.cvtColor(depth, cv2.COLOR_BGR2RGB)
  plt.subplot(221+i)
  plt.axis('off')
  plt.imshow(image)
  plt.title('Input image')
  plt.subplot(222+i)
  plt.axis('off')
  plt.imshow(depth,cmap=plt.get_cmap(cmap))
  plt.title('Ground truth depth')
  '''
  plt.imshow(depth)
  plt.title('Ground truth normal')
  '''

# Model
import tensorflow as tf
from tensorflow.keras.applications import DenseNet169
from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPool2D, Dropout, BatchNormalization
from tensorflow.keras.layers import LeakyReLU, concatenate, Concatenate, Input
from tensorflow.keras import Model

def upsampling(input_tensor, n_filters, concat_layer):
  '''
  Block of Decoder
  '''
  # Bilinear 2x upsampling layer
  x = UpSampling2D(size=(2,2), interpolation='bilinear')(input_tensor)
  # concatenation with encoder block
  x = concatenate([x,concat_layer])
  # decreasing the depth filters by half
  x = Conv2D(filters=n_filters, kernel_size=(3,3), padding='same')(x)
  x = BatchNormalization()(x)
  x = Conv2D(filters=n_filters, kernel_size=(3,3), padding='same')(x)
  x = BatchNormalization()(x)
  return x

# Layer name of encoders to be concatenated
names = ['pool3_pool', 'pool2_pool', 'pool1','conv1/relu']
# Transfer learning approach without the classification head
encoder = DenseNet169(include_top=False, weights='imagenet', input_shape=(192,640,3))
for layer in encoder.layers:
  layer.trainable = True
inputs = encoder.input
x = encoder.output
# decoder blocks linked with corresponding encoder blocks
bneck = Conv2D(filters=1664, kernel_size=(1,1), padding='same')(x)
x = LeakyReLU(alpha=0.2)(bneck)
x = upsampling(bneck, 832, encoder.get_layer(names[0]).output)
x = LeakyReLU(alpha=0.2)(x)
x = upsampling(x, 416, encoder.get_layer(names[1]).output)
x = LeakyReLU(alpha=0.2)(x)
x = upsampling(x, 208, encoder.get_layer(names[2]).output)
x = LeakyReLU(alpha=0.2)(x)
x = upsampling(x, 104, encoder.get_layer(names[3]).output)
x = LeakyReLU(alpha=0.2)(x)
x = tf.keras.layers.UpSampling2D(size=(2,2), interpolation='bilinear')(x)
x = Conv2D(filters=1,kernel_size=(3,3), padding='same')(x)

model = Model(inputs=inputs, outputs=x)
model.summary()

!pip install tensorflow_addons

from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import tensorflow.keras.backend as K
import tensorflow_addons as tfa
'''
def loss_function(y_true, y_pred):

  #Cosine distance loss
  l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)

  # edge loss for sharp edges

  dy_true, dx_true = tf.image.image_gradients(y_true)
  dy_pred, dx_pred = tf.image.image_gradients(y_pred)
  l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)

  # structural similarity loss
  l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, 1.0)) * 0.5, 0, 1)

  # weightage
  w1, w3 = 1.0, 0.1
  #+ (w2 * K.mean(l_edges)) w2=1.0
  return (w1 * l_ssim) + (w3 * K.mean(l_depth))
'''

#optimizer
opt = tfa.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-6,amsgrad=True)

# accuracy function
def accuracy_function(y_true, y_pred):
  return K.mean(K.equal(K.round(y_true), K.round(y_pred)))

# save model frequently for later use.
checkpoint = tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/project/depth_final_2',
                                                save_best_only=True,
                                                verbose=1)
# Learning rate scheduler
def polynomial_decay(epoch):
  max_epochs = 10
  base_lr =  0.0001
  power = 1.0
  lr = base_lr * (1 - (epoch / float(max_epochs))) ** power
  return lr

callbacks = [LearningRateScheduler(polynomial_decay, verbose=1), checkpoint]

for layer in encoder.layers:
  layer.trainable = True
  print(layer.trainable)

model.compile(optimizer=opt, loss='mean_squared_error', metrics=[accuracy_function])# change
history = model.fit(train_generator, validation_data=val_generator, epochs=10,callbacks=callbacks)

model.evaluate(test_generator)

# normal
def show_train_history(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title('Train History')
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
show_train_history(history, 'loss', 'val_loss')
show_train_history(history, 'accuracy_function', 'val_accuracy_function')

def show_train_history(train_history, train, validation):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title('Train History')
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
show_train_history(history, 'loss', 'val_loss')
show_train_history(history, 'accuracy_function', 'val_accuracy_function')

model.save('/content/drive/MyDrive/depth_model_to_gans.h5')

model.save_weights('/content/drive/MyDrive/my_model_weights_depth_2.h5')

images,labels = next(iter(train_generator))
preds = model.predict(images)

cmap = "plasma_r"


for i in range(len(images)):
  plt.figure(figsize=(20,10))
  plt.subplot(1,3,1)
  plt.axis("off")
  img1 = images[i]
  img1=cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)
  plt.imshow(img1)

  pred = preds[i]
  pred = np.squeeze(pred, axis=-1)
  #pred=cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)
  plt.subplot(1,3,2)
  plt.axis("off")
  plt.imshow(pred, cmap=plt.get_cmap(cmap))
  #plt.imshow(pred)

  plt.subplot(1,3,3)
  plt.axis("off")
  img = labels[i]
  img = np.squeeze(img, axis=-1)
  #img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  plt.imshow(img, cmap=plt.get_cmap(cmap))
  #plt.imshow(img)

  plt.show()

train_df = pd.read_csv('/content/data/nyu2_train.csv',header=None)
test = pd.read_csv('/content/data/nyu2_test.csv',header=None).rename(columns={0:'image', 1:'depth'})
train_df = train_df.sample(frac=1).reset_index(drop=True).rename(columns={0:'image', 1:'depth'})
train_split = int(len(train_df)*split)


train = train_df[:train_split]
validation = train_df[train_split:]
len(train), len(validation)